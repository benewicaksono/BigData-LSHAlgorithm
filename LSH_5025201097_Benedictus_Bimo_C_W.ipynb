{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignments 3**<br>\n",
        "Name: Benedictus Bimo C W<br>\n",
        "Student ID: 5025201097<br>\n",
        "Class: Big Data A<br>\n",
        "Lecturer: Abdul Munif, S.Kom., M.Sc."
      ],
      "metadata": {
        "id": "hNbdg1_LFLeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source:\n",
        "1. https://www.uber.com/en-ID/blog/lsh/\n",
        "2. https://stackoverflow.com/questions/56816537/cant-find-kaggle-json-file-in-google-colab\n",
        "3. https://spark.apache.org/docs/latest/api/python/index.html\n",
        "4. https://spark.apache.org/docs/latest/ml-features.html#locality-sensitive-hashing"
      ],
      "metadata": {
        "id": "LZoLwwbbFZ9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "yq47FAjaFvYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the Environment"
      ],
      "metadata": {
        "id": "CZVK9AkSFxIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ece9cSOFFlQ"
      },
      "outputs": [],
      "source": [
        "!java --version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Apache Spark (PySpark)"
      ],
      "metadata": {
        "id": "vW7EtjpchtnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Installing Apache Spark (PySpark)\n",
        "\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "H8J1WVeHhuz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Apache Spark context"
      ],
      "metadata": {
        "id": "lbamGvWtiYOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Apache Spark SQL\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session/Context\n",
        "# We are using local machine with all the CPU cores [*]\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Hello Pyspark\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "E-2Xe9CaiX7K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check spark session\n",
        "print(spark)"
      ],
      "metadata": {
        "id": "WQ_DpqwNiclC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Mining Task"
      ],
      "metadata": {
        "id": "SelrGM4DOGuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LSH task always consists of three steps:\n",
        "\n",
        "1. Converting original data into vectors\n",
        "2. Calculate the hash using MinHash algorithm\n",
        "3. Searching the similar pairs using k-Nearest Neighbor, or join algorithm."
      ],
      "metadata": {
        "id": "hwc98F0BOHsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dataset"
      ],
      "metadata": {
        "id": "BKR5l4TPOJsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "Li3dBYaTOLEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLEASE USE YOUR OWN KEY\n",
        "# Download your own key according to this instruction https://github.com/Kaggle/kaggle-api#api-credentials\n",
        "\n",
        "import json\n",
        "api_token = {\"username\": \"benewicaksono\",\n",
        "             \"key\": \"c266a24ed937b36cbf606754380a80b5\"}\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "El-aEUWxQjo6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download from https://www.kaggle.com/datasets/urbanbricks/wikipedia-promotional-articles\n",
        "\n",
        "!kaggle datasets download -d urbanbricks/wikipedia-promotional-articles"
      ],
      "metadata": {
        "id": "8bPIAgE9RG7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dataset"
      ],
      "metadata": {
        "id": "iOd4g7yoTXDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wikipedia-promotional-articles.zip"
      ],
      "metadata": {
        "id": "mOE-IJqpTbBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check files in current directory"
      ],
      "metadata": {
        "id": "miucMnDuTgdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "cPCDKY4_TiB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "q0RSGu6_TrZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV (promotional.csv)\n",
        "df = spark.read.option(\"header\", True).csv(\"/content/promotional.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "Q5OeENKeTsHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an ID for the dataset\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "newsDF = df.withColumn(\"id\", monotonically_increasing_id())\n",
        "newsDF.show()"
      ],
      "metadata": {
        "id": "Rg80gHKRUXbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the totals row\n",
        "newsDF.count()"
      ],
      "metadata": {
        "id": "veDHR-TtUhqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare the tokenizer\n",
        "We transform the input into tokenized words"
      ],
      "metadata": {
        "id": "Qga7TndeUmcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the tokenizer\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# create a tokenizer object to tokenize the text\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "# tokenize the text in the dataframe\n",
        "wordsDF = tokenizer.transform(newsDF)\n",
        "\n",
        "# show the resulting dataframe\n",
        "wordsDF.show()"
      ],
      "metadata": {
        "id": "rKfN93bGU1ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the dataset\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# define the size of the vocabulary and the minimum document frequency\n",
        "vocabSize=1000\n",
        "\n",
        "# create a CountVectorizer object and fit it on the tokenized data\n",
        "cvModel = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=vocabSize, minDF=10).fit(wordsDF)\n",
        "\n",
        "# transform the tokenized data into a vectorized format\n",
        "vectorizedDF = cvModel.transform(wordsDF)\n",
        "\n",
        "# show the resulting dataframe\n",
        "vectorizedDF.show()"
      ],
      "metadata": {
        "id": "en2AZSpPU5I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fit/train an LSH Model"
      ],
      "metadata": {
        "id": "p5YRn-GaU-Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinHashLSH\n",
        "\n",
        "# Define the MinHashLSH model with the desired input and output columns, and number of hash tables\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", numHashTables=3)\n",
        "\n",
        "# Train the model using the vectorized data\n",
        "LSHmodel = mh.fit(vectorizedDF)\n",
        "\n",
        "# Apply the trained LSH model to the vectorized data and show the results\n",
        "LSHmodel.transform(vectorizedDF).show()\n"
      ],
      "metadata": {
        "id": "1-0MX0iUVADQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Searching the similar pairs/items for a key \"united\" \"states\""
      ],
      "metadata": {
        "id": "A8HSOJoSVhIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index of the word \"united\" and \"states\" in the vocabulary\n",
        "print(cvModel.vocabulary.index(\"united\"))\n",
        "print(cvModel.vocabulary.index(\"states\"))"
      ],
      "metadata": {
        "id": "6cdM9T6jVkHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input with 2 words into a 1000-size vector\n",
        "# If the words exist in the index, we will give the value 1.0, otherwise 0.0\n",
        "# Final result: key = [0, 0, ..., 1.0, ..., 1.0, 0, ..., 0]\n",
        "from pyspark.ml.linalg import Vectors\n",
        "key = Vectors.sparse(vocabSize, {cvModel.vocabulary.index(\"united\"): 1.0, cvModel.vocabulary.index(\"states\"): 1.0})"
      ],
      "metadata": {
        "id": "e8UMBKH0V7EA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of neighbors\n",
        "k = 40"
      ],
      "metadata": {
        "id": "hfrenxraV8-6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search inside the LSH model that we already trained\n",
        "resultDF = LSHmodel.approxNearestNeighbors(vectorizedDF, key, k)\n",
        "resultDF.show()"
      ],
      "metadata": {
        "id": "XVtpXSfVWFs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into CSV\n",
        "import pandas as pd\n",
        "data = resultDF.toPandas()\n",
        "data.to_csv(\"result.csv\")"
      ],
      "metadata": {
        "id": "oBsiNvLxWVJQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Result.csv"
      ],
      "metadata": {
        "id": "gfK4uhI5XDt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read the CSV file into a Pandas dataframe\n",
        "df = pd.read_csv('result.csv')\n",
        "\n",
        "# display the first 5 rows of the dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "CW1LJhH6XGPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}